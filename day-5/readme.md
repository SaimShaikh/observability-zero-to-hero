
# Kubernetes Logging with EFK Stack - Complete Guide

A beginner-friendly, comprehensive guide to understand Kubernetes logging, why you need it, and how to implement the EFK stack step-by-step.

---

## Table of Contents
1. [Problem Scenario](#problem-scenario)
2. [What is Logging in Kubernetes?](#what-is-logging-in-kubernetes)
3. [Why Do You Need Logging?](#why-do-you-need-logging)
4. [Benefits of Centralized Logging](#benefits-of-centralized-logging)
5. [What is the EFK Stack?](#what-is-the-efk-stack)
6. [EFK Stack Components](#efk-stack-components)
7. [Architecture Overview](#architecture-overview)
8. [Step-by-Step Setup Guide](#step-by-step-setup-guide)
9. [Verification and Testing](#verification-and-testing)
10. [Troubleshooting](#troubleshooting)
11. [Cleanup](#cleanup)

---

## Problem Scenario

Imagine you're running a **microservices application on Kubernetes** with 50 different services. Something goes wrong with one service, and users start complaining.

### The Problem (Without Centralized Logging)

**Without Logging:**

```
User reports: "Application is slow!"

You start debugging:
❌ Is it the database service? Need to SSH into that pod
❌ Is it the API service? Need to SSH into another pod
❌ Is it the payment service? Need to check yet another pod
❌ You're running from pod to pod, checking logs manually
❌ Logs are scattered everywhere - on different nodes
❌ Old logs are deleted after a few days
❌ No way to correlate which service caused the problem
❌ Hours of debugging lost time!
```

**Real Issues:**

- Each pod stores logs only locally
- When a pod crashes, logs are deleted automatically
- No way to search logs across all pods
- Each developer checks logs differently
- You can't see the complete picture of what happened
- Impossible to trace a request through multiple microservices

### How Centralized Logging Solves This

**With EFK Stack Logging:**

```
User reports: "Application is slow!"

With EFK Stack:
✅ All logs from all pods go to one central place (Elasticsearch)
✅ Search across all services at once with Kibana
✅ See exact timeline of what happened
✅ Track how a request moved through 5 different services
✅ Identify the exact service that caused the problem
✅ See metrics, errors, and debug info all together
✅ Problem solved in 15 minutes instead of hours!
```

**Real Benefits:**

- One central location for all logs
- Search logs from 1000+ pods instantly
- See logs from multiple services together
- Historical data kept for weeks/months
- Trace request flow across services
- Visual dashboards for monitoring

---

## What is Logging in Kubernetes?

**Logging in Kubernetes** is the process of collecting, storing, and analyzing logs generated by applications and infrastructure running in a Kubernetes cluster.[94][97]

### Simple Definition

Think of Kubernetes logging like a **library for events**:
- Every pod, service, and component writes down what happened
- Instead of throwing these logs away, we collect them in one library
- We organize them, index them, and make them searchable
- We can find any event instantly using search tools

### Why Logs Matter in Kubernetes[94][96]

```
Application Output → Pod Logs → Collected by Log Forwarder → Stored Centrally → Searchable & Analyzable
```

Every time your application:
- Handles a request
- Encounters an error
- Reaches a milestone
- Connects to a database
- Processes data

...it generates a **log entry**. Without logging, this information is lost forever!

### Types of Logs in Kubernetes[102]

1. **Application Logs**
   - Logs from your microservices
   - Business logic errors and events
   - Request/response information

2. **System Logs**
   - Kubernetes control plane logs
   - Node logs (kernel, services)
   - Container runtime logs

3. **Audit Logs**
   - Who accessed what
   - Configuration changes
   - Security events

---

## Why Do You Need Logging?

### 1. Debugging Issues[94][96]

**Scenario:** A user reports that they can't upload files

**Without Logging:**
```
You: "Is it the upload service?"
You check the pod... no error messages
You check the database... looks fine
You check the storage... seems okay
30 minutes later: "Give me a specific error message!"
```

**With Logging:**
```
You search Kibana: "upload failed"
Result: "Storage quota exceeded in S3 bucket"
Problem solved in 1 minute!
```

### 2. Performance Monitoring[94]

**Find slow services:**
- Which API endpoint takes 5 seconds to respond?
- Which database query is slow?
- Which service is consuming most resources?

### 3. Security & Compliance[94]

Track:
- Who accessed sensitive data?
- Were there unauthorized login attempts?
- Did configuration change unexpectedly?
- Are we meeting audit requirements?

### 4. Troubleshooting Distributed Systems[99]

In microservices, a single user request flows through multiple services:

```
User Request
    ↓
API Gateway (logs: request received)
    ↓
Authentication Service (logs: user verified)
    ↓
Order Service (logs: order created)
    ↓
Payment Service (logs: payment processed)
    ↓
Email Service (logs: confirmation sent)
```

**Problem:** Payment service fails

**Without Logging:**
- No idea where the request got stuck
- Can't trace request through all services

**With Logging:**
- See the exact flow: user → API → Auth → Order → Payment (FAILED)
- Immediately know it's the payment service
- Read payment service logs to find the exact error

### 5. Historical Data & Trends[100]

Learn:
- Peak traffic times (for capacity planning)
- Most common errors (to prioritize fixes)
- Performance trends over time
- Whether changes improved things

---

## Benefits of Centralized Logging

### 1. Single Source of Truth[94][97]

Instead of checking multiple pods:
```
❌ SSH into pod-1 → read logs
❌ SSH into pod-2 → read logs
❌ SSH into pod-3 → read logs
(and so on...)

✅ Search all pods at once in Kibana
```

### 2. Searchable & Filterable[94]

Search for anything instantly:
- `"ERROR"` - all errors
- `"status=500"` - all failures
- `"response_time > 2000"` - all slow requests
- `"user_id=12345"` - all events for specific user

### 3. Long-Term Storage[100]

Keep logs for weeks or months:
- Old logs available for historical analysis
- Compliance requirements met
- Can correlate old issues with recent ones

### 4. Visual Dashboards[97]

See at a glance:
- How many errors in the last hour?
- Average response time trending up or down?
- Which service has most errors?
- What % of requests are failing?

### 5. Alerting & Monitoring[94]

Automatic alerts:
- Alert when error rate exceeds 5%
- Notify when specific service crashes
- Send warning when disk space low
- Page on-call engineer for critical issues

### 6. Multi-Team Collaboration[97]

Everyone sees the same data:
- Developers see application errors
- DevOps sees infrastructure issues
- Security team sees access logs
- Managers see performance metrics

All in one place, no confusion!

---

## What is the EFK Stack?

**EFK Stack** is a popular open-source solution for collecting, storing, and visualizing logs in Kubernetes.[95][98]

### Simple Explanation

Think of EFK like a **Post Office System**:

- **Fluentbit** = Postal workers collecting letters (logs) from mailboxes (pods)
- **Elasticsearch** = Central mail sorting facility storing all letters
- **Kibana** = Post office window where you search and read letters

### Why EFK Stack?[98]

- **Open Source** - Free to use and modify
- **Scalable** - Handles thousands of pods easily
- **Popular** - Used by Netflix, Uber, and other major companies
- **Mature** - Been around for years, well-documented
- **Flexible** - Works with any Kubernetes cluster

### Comparison: EFK vs Other Logging Options

| Feature | EFK | ELK | Loki Stack |
|---------|-----|-----|-----------|
| **Elasticsearch** | ✅ | ✅ | ❌ |
| **Logstash** | ❌ | ✅ | ❌ |
| **Fluentbit** | ✅ | ❌ | ❌ |
| **Fluentd** | ❌ | ❌ | ❌ |
| **Kibana** | ✅ | ✅ | ❌ (Grafana instead) |
| **Loki** | ❌ | ❌ | ✅ |
| **Cost** | Free | Free | Free |
| **Resource Usage** | Medium | High | Low |
| **Best For** | Production | Complex logging | Lightweight |

---

## EFK Stack Components

### Component 1: Fluentbit (Log Collector)[98]

**What it does:**
Fluentbit runs on every node and **collects logs from all pods**.

**Think of it as:**
A postal worker with a bicycle collecting letters from mailboxes on every street (node).

**How it works:**

```
Pod A logs → Fluentbit reads → Forward to Elasticsearch
Pod B logs → Fluentbit reads → Forward to Elasticsearch
Pod C logs → Fluentbit reads → Forward to Elasticsearch
```

**Key Features:[104]**

- **Lightweight** - Uses only ~30MB RAM per pod
- **Fast** - Low latency (quick log forwarding)
- **Reliable** - Doesn't lose logs even if connection fails
- **Flexible** - Can filter, parse, and enrich logs before sending

**Fluentbit vs Fluentd:**

| Aspect | Fluentbit | Fluentd |
|--------|-----------|---------|
| **Size** | Small (~30MB) | Larger (~500MB) |
| **Speed** | Very fast | Slower |
| **Resources** | Light | Heavy |
| **Plugins** | ~45 | ~700 |
| **Best For** | Kubernetes edge | Complex processing |
| **Language** | C (fast) | Ruby (flexible) |

**For Kubernetes:** Fluentbit is the better choice!

### Component 2: Elasticsearch (Log Storage)[98]

**What it does:**
Elasticsearch **stores and indexes all logs** so they can be searched instantly.

**Think of it as:**
A huge library with a sophisticated card catalog that lets you find any book instantly.

**How it works:**

```
Fluentbit sends logs to Elasticsearch
         ↓
Elasticsearch indexes logs
         ↓
Logs searchable in milliseconds
```

**Key Features:**

- **Powerful Search** - Find logs using complex queries
- **Indexing** - Organizes logs for instant retrieval
- **Scalability** - Handles logs from 1000+ pods
- **Retention** - Keep logs for weeks/months
- **Analysis** - Built-in aggregations and statistics

**How Elasticsearch Stores Logs:**

```
Log Entry:
{
  "timestamp": "2025-12-10T10:30:45Z",
  "pod_name": "api-server-1",
  "container": "application",
  "level": "ERROR",
  "message": "Database connection failed",
  "host": "node-2"
}

Indexed as:
{
  Timestamp: 10:30:45
  Pod: api-server-1
  Level: ERROR
  Message: "Database..."
  Host: node-2
}

Now searchable by any field!
```

### Component 3: Kibana (Log Visualization)[98]

**What it does:**
Kibana provides a **beautiful web interface** to search, analyze, and visualize logs.

**Think of it as:**
A customer service desk where you can ask any question about the logs and get beautiful charts and graphs as answers.

**What you can do with Kibana:**

1. **Search Logs**
   ```
   Search: "error"
   Result: All error logs from all pods
   ```

2. **Filter Logs**
   ```
   Show me only logs from production environment
   Show me only errors from last hour
   Show me only requests taking > 2 seconds
   ```

3. **Visualize Data**
   - Line charts showing error rate over time
   - Bar charts showing errors by service
   - Pie charts showing request distribution
   - Heat maps showing peak traffic times

4. **Create Dashboards**
   - Real-time dashboard showing cluster health
   - Dashboard showing top 10 errors
   - Dashboard showing performance metrics

5. **Set Alerts**
   - Alert if error rate > 5%
   - Alert if service returns 500 errors
   - Alert if response time > 3 seconds

---

## Architecture Overview

### High-Level Architecture

![Project Architecture](images/architecture.gif)

### Data Flow

```
1. Application writes log → stdout
2. Pod captures log in /var/log/containers/
3. Fluentbit reads log file continuously
4. Fluentbit adds metadata (pod name, namespace, etc.)
5. Fluentbit sends log to Elasticsearch
6. Elasticsearch indexes log for search
7. You search in Kibana
8. Kibana queries Elasticsearch
9. Beautiful results shown in Kibana UI
```

### Where Logs Go

```
Pod A ──┐
Pod B ──┤
Pod C ──┼──→ Fluentbit ──→ Elasticsearch ──→ Kibana (Search & Visualize)
Pod D ──┤
...    ──┘
```

---

## Step-by-Step Setup Guide

### Prerequisites

You need:
- ✅ AWS EKS Cluster (or any Kubernetes cluster)
- ✅ kubectl installed and configured
- ✅ Helm package manager installed
- ✅ Basic understanding of Kubernetes
- ✅ AWS IAM permissions (for EBS CSI driver)

### Step 1: Create IAM Role for EBS CSI Driver

**What is this doing?**

Elasticsearch needs persistent storage (disk space) to store logs. We're creating an AWS IAM role that allows Kubernetes to create and manage AWS EBS volumes.

**Think of it as:**
Giving Kubernetes permission to buy and manage hard drives from AWS.

**Command:**

```bash
eksctl create iamserviceaccount \
    --name ebs-csi-controller-sa \
    --namespace kube-system \
    --cluster observability \
    --role-name AmazonEKS_EBS_CSI_DriverRole \
    --role-only \
    --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \
    --approve
```

**What happens:**
- Creates an IAM role named `AmazonEKS_EBS_CSI_DriverRole`
- Attaches AWS policies to allow EBS management
- Links this role to Kubernetes service account

**Real-world analogy:**
Like giving your accountant a credit card with EBS service authorization.

### Step 2: Get the IAM Role ARN

**What is this doing?**

We retrieve the unique identifier (ARN) of the role we just created. We'll use this in the next step.

**Command:**

```bash
ARN=$(aws iam get-role --role-name AmazonEKS_EBS_CSI_DriverRole --query 'Role.Arn' --output text)

# Verify it worked
echo $ARN
# Output should look like: arn:aws:iam::123456789012:role/AmazonEKS_EBS_CSI_DriverRole
```

**What happens:**
- Queries AWS for the role we created
- Stores the ARN in a variable `$ARN`
- We'll use `$ARN` in the next command

### Step 3: Deploy EBS CSI Driver

**What is this doing?**

We're installing the EBS CSI driver, which is an addon that manages storage volumes in Kubernetes.

**Command:**

```bash
eksctl create addon \
    --cluster observability \
    --name aws-ebs-csi-driver \
    --version latest \
    --service-account-role-arn $ARN \
    --force
```

**What happens:**
- Installs the EBS CSI driver addon in your cluster
- Uses the IAM role from Step 1
- Allows creation of persistent volumes for Elasticsearch

**Wait for completion:**
```bash
# Check if addon is ready
eksctl get addon --cluster observability

# Look for: aws-ebs-csi-driver  ACTIVE
```

### Step 4: Create Logging Namespace

**What is this doing?**

We're creating a separate namespace to keep all logging components organized.

**Command:**

```bash
kubectl create namespace logging
```

**What happens:**
- Creates a namespace called `logging`
- All EFK components will be installed here
- Keeps logging separate from application code

**Verify:**

```bash
kubectl get namespace logging
# Output: NAME      STATUS   AGE
#         logging   Active   1s
```

### Step 5: Add Helm Repository for Elasticsearch

**What is this doing?**

Helm is a package manager for Kubernetes. We're telling Helm where to find Elasticsearch packages.

**Command:**

```bash
helm repo add elastic https://helm.elastic.co
```

**What happens:**
- Adds Elastic's official Helm repository
- Downloads the list of available packages
- Now we can install Elasticsearch easily

**Update Helm (optional but recommended):**

```bash
helm repo update
```

### Step 6: Install Elasticsearch

**What is this doing?**

Installing Elasticsearch in the `logging` namespace. This is where all logs will be stored.

**Command:**

```bash
helm install elasticsearch \
    --set replicas=1 \
    --set volumeClaimTemplate.storageClassName=gp2 \
    --set persistence.labels.enabled=true \
    elastic/elasticsearch \
    --namespace logging
```

**What each parameter means:**

- `--set replicas=1`: Run one instance (use 3+ in production for redundancy)
- `--set volumeClaimTemplate.storageClassName=gp2`: Use AWS gp2 storage (general purpose)
- `--set persistence.labels.enabled=true`: Enable labels for organization
- `--namespace logging`: Install in the logging namespace

**Wait for Elasticsearch to be ready:**

```bash
# Check status
kubectl get pods -n logging

# Wait until elasticsearch-master-0 shows READY 1/1
kubectl wait --for=condition=ready pod \
    -l app=elasticsearch -n logging --timeout=300s
```

**Expected output:**

```
NAME                          READY   STATUS    RESTARTS   AGE
elasticsearch-master-0        1/1     Running   0          2m
```

### Step 7: Get Elasticsearch Credentials

**What is this doing?**

Elasticsearch has default credentials stored in a Kubernetes secret. We need to retrieve them to access it.

**Get Username:**

```bash
kubectl get secrets \
    --namespace=logging \
    elasticsearch-master-credentials \
    -ojsonpath='{.data.username}' | base64 -d
```

**Get Password:**

```bash
kubectl get secrets \
    --namespace=logging \
    elasticsearch-master-credentials \
    -ojsonpath='{.data.password}' | base64 -d
```

**Save these credentials!**

```
Username: elastic
Password: (copy from above, e.g., "NJyO47UqeYBsoaEU")
```

### Step 8: Install Kibana

**What is this doing?**

Installing Kibana, which provides the web interface for searching and visualizing logs.

**Command:**

```bash
helm install kibana \
    --set service.type=LoadBalancer \
    elastic/kibana \
    --namespace logging
```

**What parameters mean:**

- `--set service.type=LoadBalancer`: Expose Kibana externally (AWS creates a load balancer)

**Get the Kibana URL:**

```bash
kubectl get svc -n logging

# Look for kibana service
# NAME      TYPE           CLUSTER-IP       EXTERNAL-IP              PORT(S)
# kibana    LoadBalancer   10.100.200.100   a1234567890.elb.amazonaws.com   5601:30210/TCP
```

**Your Kibana URL:**
```
http://a1234567890.elb.amazonaws.com:5601
```

### Step 9: Create Fluentbit Values File

**What is this doing?**

Creating a configuration file for Fluentbit with our Elasticsearch credentials.

**Create file `fluentbit-values.yaml`:**

```yaml
# Fluentbit configuration
config:
  service: |
    [SERVICE]
        Flush         5
        Daemon        off
        Log_Level     info

  inputs: |
    [INPUT]
        Name              tail
        Path              /var/log/containers/*.log
        Parser            docker
        Tag               kube.*
        Refresh_Interval  5
        Skip_Long_Lines   On

  filters: |
    [FILTER]
        Name                kubernetes
        Match               kube.*
        Kube_URL            https://kubernetes.default.svc:443
        Kube_CA_File        /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        Kube_Token_File     /var/run/secrets/kubernetes.io/serviceaccount/token
        Kube_Tag_Prefix     kube.var.log.containers.
        Merge_Log           On
        Keep_Log            Off

  outputs: |
    [OUTPUT]
        Name            es
        Match           *
        Host            elasticsearch-master
        Port            9200
        HTTP_User       elastic
        HTTP_Passwd     NJyO47UqeYBsoaEU
        Logstash_Format On
        Retry_Limit     5
        Type            _doc
```

**⚠️ IMPORTANT:** Replace `NJyO47UqeYBsoaEU` with the actual password you got in Step 7!

### Step 10: Add Fluentbit Helm Repository

**What is this doing?**

Adding Fluentbit's Helm repository so we can install it.

**Command:**

```bash
helm repo add fluent https://fluent.github.io/helm-charts
helm repo update
```

### Step 11: Install Fluentbit

**What is this doing?**

Installing Fluentbit to collect logs from all pods and send them to Elasticsearch.

**Command:**

```bash
helm install fluent-bit fluent/fluent-bit \
    -f fluentbit-values.yaml \
    --namespace logging
```

**Wait for Fluentbit to be ready:**

```bash
kubectl get pods -n logging

# Should show fluent-bit pods on each node
kubectl get daemonset -n logging
```

**Expected output:**

```
NAME         DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE
fluent-bit   3         3         3       3            3
```

---

## Verification and Testing

### Step 1: Verify All Components Running

**Check all pods:**

```bash
kubectl get pods -n logging
```

**Expected output:**

```
NAME                            READY   STATUS    RESTARTS   AGE
elasticsearch-master-0          1/1     Running   0          10m
kibana-65f4d8c9d4-abc123       1/1     Running   0          5m
fluent-bit-abc12               1/1     Running   0          2m
fluent-bit-def45               1/1     Running   0          2m
fluent-bit-ghi67               1/1     Running   0          2m
```

### Step 2: Access Kibana

**Get Kibana URL:**

```bash
kubectl get svc kibana -n logging

# Copy the EXTERNAL-IP and add :5601
```

**Open in browser:**

```
http://EXTERNAL-IP:5601
```

### Step 3: Login to Kibana

Use credentials from Step 7:
- **Username:** elastic
- **Password:** (the one you retrieved)

### Step 4: Create Data View in Kibana

**What is this doing?**

Kibana needs to know which Elasticsearch indices to search. We're telling it to search logs from today.

**Steps in Kibana UI:**

1. Go to **Management** → **Data Views**
2. Click **Create data view**
3. **Name:** `logs-*`
4. **Timestamp field:** `@timestamp`
5. Click **Create data view**

### Step 5: Explore Logs

**In Kibana:**

1. Click **Discover** (on left sidebar)
2. Select `logs-*` data view
3. You should see logs flowing in!

**Try these searches:**

```
# All errors
level: ERROR

# All logs from last 1 hour
@timestamp > now-1h

# Logs from specific pod
kubernetes.pod_name: "api-server-1"

# Logs with specific keyword
message: "database"
```

### Step 6: Create a Dashboard

**In Kibana:**

1. Click **Dashboard**
2. Click **Create dashboard**
3. Click **Add a panel**
4. Click **Create visualization**
5. Choose visualization type (Line chart, Bar chart, etc.)
6. Search for logs you want to visualize
7. Click **Save**

**Example dashboards:**

- Error rate over time
- Requests per service
- Response time trends
- Top 10 error messages

---

## Troubleshooting

### Issue 1: Fluentbit pods not running

**Problem:** Fluentbit pods show `CrashLoopBackOff`

**Solution:**

```bash
# Check logs
kubectl logs -n logging -l app.kubernetes.io/name=fluent-bit --tail=50

# Likely cause: Wrong Elasticsearch password
# Fix: Update fluentbit-values.yaml with correct password
# Then upgrade the release
helm upgrade fluent-bit fluent/fluent-bit \
    -f fluentbit-values.yaml \
    --namespace logging
```

### Issue 2: No logs appearing in Kibana

**Problem:** Kibana shows no data

**Solution:**

```bash
# Check if Fluentbit is collecting logs
kubectl logs -n logging fluent-bit-abc123

# Check Elasticsearch storage
kubectl exec -it elasticsearch-master-0 -n logging -- bash
> curl -u elastic:PASSWORD http://localhost:9200/_cat/indices

# Should see indices like: logstash-2025.12.10
```

### Issue 3: Kibana not accessible

**Problem:** Can't connect to Kibana URL

**Solution:**

```bash
# Verify service is LoadBalancer type
kubectl get svc kibana -n logging

# Wait longer for AWS to provision load balancer (5-10 minutes)
# Check service status
kubectl describe svc kibana -n logging

# If EXTERNAL-IP is pending, wait a bit more
```

### Issue 4: Elasticsearch out of disk space

**Problem:** Elasticsearch not accepting new logs

**Solution:**

```bash
# Check disk usage
kubectl exec -it elasticsearch-master-0 -n logging -- bash
> df -h

# Increase storage size
# Edit Elasticsearch persistent volume claim
kubectl edit pvc -n logging

# Or delete old indices
kubectl exec -it elasticsearch-master-0 -n logging -- bash
> curl -u elastic:PASSWORD -X DELETE http://localhost:9200/logstash-2025.12.01
```

### Issue 5: High memory usage

**Problem:** Elasticsearch consuming too much memory

**Solution:**

```bash
# Check memory limits
kubectl get pod elasticsearch-master-0 -n logging -o yaml | grep -A 5 limits

# Update Helm values to limit memory
helm upgrade elasticsearch \
    --set resources.limits.memory=2Gi \
    elastic/elasticsearch \
    --namespace logging
```

---

## Best Practices for Kubernetes Logging

### 1. Use Structured Logging in Applications[94]

**Bad:**
```
"Connection error"
"Failed to connect"
"Timeout occurred"
```

**Good:**
```json
{
  "timestamp": "2025-12-10T10:30:45Z",
  "level": "ERROR",
  "service": "user-service",
  "event": "database_connection_failed",
  "database": "postgresql",
  "error_code": "CONN_TIMEOUT",
  "retry_count": 3
}
```

### 2. Include Contextual Information[94]

Always include:
- Timestamp
- Request ID (for tracing through services)
- User ID (who triggered this?)
- Service name
- Environment (prod, staging, dev)

### 3. Never Log Sensitive Data[94]

❌ DON'T log:
- Passwords
- API keys
- Credit card numbers
- Personal information (PII)
- Database connection strings with passwords

✅ DO log:
- User ID (not password)
- API endpoint (not the API key)
- Reference to the transaction (not the amount)

### 4. Use Log Levels Appropriately[94]

- **DEBUG:** Detailed info for developers (verbose)
- **INFO:** General info (application started, user logged in)
- **WARNING:** Something unusual (retry happening, cache miss)
- **ERROR:** Something failed (connection failed, validation error)
- **CRITICAL:** System down, urgent action needed

### 5. Implement Log Rotation[100]

Keep logs for an appropriate time:
- **Short-term (3-7 days):** High-volume, low-value logs
- **Medium-term (30 days):** Application logs
- **Long-term (1 year):** Compliance and audit logs

### 6. Set Up Alerts[94]

Create alerts for:
- Error rate > 5%
- Response time > 3 seconds
- Service unreachable
- Out of memory
- Disk space low

### 7. Use Labels and Annotations[100]

Label your logs:
```yaml
labels:
  environment: production
  service: api-server
  version: v1.2.3
  team: backend
```

---

## Performance Considerations

### Memory Usage

- **Elasticsearch:** ~2GB (more with more logs)
- **Kibana:** ~500MB
- **Fluentbit:** ~30MB per pod

### Disk Space

- **Elasticsearch:** Depends on log volume
  - 100 pods = ~10GB per day
  - 1000 pods = ~100GB per day
- Keep logs for: 30-90 days typically

### CPU Usage

- **Elasticsearch:** Medium (increases with search complexity)
- **Kibana:** Low
- **Fluentbit:** Very low (~5% per pod)

### Network Impact

- Fluentbit sends logs continuously
- Typical: 1-10MB per pod per day
- Minimal impact on cluster network

---

## Cleanup

### Remove All EFK Components

```bash
# Uninstall Helm releases
helm uninstall kibana -n logging
helm uninstall elasticsearch -n logging
helm uninstall fluent-bit -n logging

# Delete logging namespace
kubectl delete namespace logging

# Delete EBS CSI addon (if not needed for other services)
eksctl delete addon --cluster observability --name aws-ebs-csi-driver

# Delete IAM role (if not needed)
aws iam delete-role --role-name AmazonEKS_EBS_CSI_DriverRole

# Delete EKS cluster (if completely done)
eksctl delete cluster --name observability
```

### Delete Specific Resources

```bash
# Just delete Fluentbit (keep Elasticsearch)
helm uninstall fluent-bit -n logging

# Just delete Kibana (keep Elasticsearch)
helm uninstall kibana -n logging

# Delete specific persistent volumes
kubectl delete pvc -n logging --all
```

---

## Summary

### What You've Set Up

✅ **Elasticsearch** - Centralized log storage  
✅ **Kibana** - Beautiful log visualization  
✅ **Fluentbit** - Lightweight log collection  

### What You Can Now Do

✅ Collect logs from all 1000+ pods in one place  
✅ Search logs instantly  
✅ Create dashboards showing system health  
✅ Set up alerts for critical issues  
✅ Debug problems in minutes instead of hours  
✅ Maintain compliance with audit trails  

### Benefits Gained

- **Visibility:** See what's happening in entire cluster
- **Speed:** Find problems and fix them faster
- **Reliability:** Detect issues before users notice
- **Compliance:** Maintain audit trails and regulatory requirements
- **Insights:** Understand your system better

### Next Steps

1. **Create more dashboards** - Visualize your specific applications
2. **Set up alerting** - Notify team on critical issues
3. **Fine-tune retention** - Keep logs long enough but not too long (cost)
4. **Add custom parsing** - Extract more useful information from logs
5. **Train your team** - Teach everyone how to use Kibana for debugging

---

## Additional Resources

### Kubernetes Logging Docs
- [Kubernetes Logging Architecture](https://kubernetes.io/docs/concepts/cluster-administration/logging/)

### EFK Stack Documentation
- [Elasticsearch](https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html)
- [Kibana](https://www.elastic.co/guide/en/kibana/current/index.html)
- [Fluentbit](https://docs.fluentbit.io/manual/)

### Best Practices
- Use structured logging (JSON format)
- Avoid logging sensitive data
- Include request IDs for tracing
- Monitor disk space and retention
- Regular backup of critical logs
- Separate environments (prod, staging, dev)

---

## Conclusion

The EFK stack is the industry-standard solution for logging in Kubernetes. It provides:

- **Centralized collection** of logs from all pods
- **Powerful search** to find any log instantly
- **Beautiful visualization** to understand system behavior
- **Reliable storage** to keep logs for weeks/months
- **Team collaboration** so everyone sees the same data

With logging in place, you can now **monitor, debug, and optimize** your Kubernetes applications with confidence!
